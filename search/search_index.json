{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Data Mining for Business Analytics MSBA 511: University of San Diego - Spring 2025 <p>Day/Time: Mon 4:00PM - 6:50PM Location: KCBE-104</p>"},{"location":"#about-the-course","title":"About the Course","text":"<p>Data Mining for Business Analytics is designed to equip you with the knowledge and tools necessary to extract meaningful insights from large and complex datasets to inform business decisions. This course introduces fundamental data mining techniques, including association analysis and collaborative filtering, clustering, classification, and text mining with a focus on their practical applications in solving real-world business challenges. You will learn how to prepare and preprocess data, build and evaluate predictive models, and translate analytical findings into actionable strategies. Through hands-on conceptual homework, programming, and case study/group project assignments, you will explore applications such as market basket analysis, customer segmentation, targeted marketing, and fraud detection.</p>"},{"location":"#instructor-background","title":"Instructor Background","text":"<p>With over 20 years of experience in Analytics roles across the financial services and banking industries, my primary objective is to bring real-world expertise into the classroom. I currently serve as a Principal Analytics Consultant at one of the largest banks in the U.S., where I am responsible for combating deposit fraud across more than 5,000 branches nationwide. I also mentor aspiring professionals transitioning into the Data Analytics field and teach at MiraCosta College and here at the University of San Diego. As an Analytics practitioner, I rely on data mining concepts and tools for decision-making every day. My passion is helping students like you uncover how data drives business success and empowering you to apply these concepts in real-world scenarios. I hold a Bachelor of Science in Business Management from the University of Maryland Global Campus and an MBA from Arizona State University. I\u2019m excited to share my knowledge and guide you through the practical application of data mining in the business world.</p>"},{"location":"#creative-commons","title":"Creative Commons","text":"<p>Unless otherwise noted, the materials on this website and repository are freely available under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.</p>"},{"location":"calendar/","title":"Calendar","text":""},{"location":"calendar/#schedule","title":"Schedule","text":"<p>The table below links to notes, reading, and homework for each week. The topics and order are subject to change as the course progresses.</p> Week / Topic(s) Class Date Optional Reading Required Reading Assignments 1. Introduction &amp; Association Rules 2/3/2025 1-2 14.1 HW1 2. Collaborative Filtering 2/10/2025 14.2 PA1 3. Cluster Analysis 2/17/2025 3-4 15 HW2 4. Classification / Decision Trees 2/24/2025 9.1-9.5 PA2 5. Evaluating Classifiers / Exam Review 3/3/2025 5 HW3 Spring Break: (March 10-14: No Classes) 6. Exam 1 3/17/2025 PA3 7. KNN / Naive Bayes Classifier 3/24/2025 7 - 8 HW4 8. Logistic Regression 3/31/2025 10 PA4 9. Random Forest / Boosted Trees 4/7/2025 9.7 - 9.9 HW5 10. Ensembles 4/14/2025 13 Project Proposal Easter Holiday: (April 17-21: No Classes) 11. Hackathon 4/28/2025 12. Text Mining / Project Work 5/5/2025 20 HW6 13. Project Presentations / Exam Review 5/12/2025 Final Project 14. Exam 2 5/19/2025"},{"location":"calendar/#important-dates","title":"Important Dates","text":"<p>See Academic Calendar</p> <ul> <li>January 30: Classes Begin </li> <li>March 10-14: Spring Break </li> <li>April 17-21: Easter Holiday </li> <li>May 16: Last Day of Classes </li> <li>May 17-18: Study Days </li> <li>May 19-23: Final Examinations </li> <li>May 24-25: Commencement </li> <li>May 26: Memorial Day</li> </ul>"},{"location":"links/","title":"Links","text":"<p>Below are a list of helpful external links for the course:</p> <ul> <li>Canvas</li> <li>Gradescope</li> <li>Course GitHub Repository</li> <li>Data Mining Examples Repository</li> </ul>"},{"location":"syllabus/","title":"Syllabus","text":"MSBA 511: Data Mining for Business Analytics University of San Diego | Spring 2025: 3 Units"},{"location":"syllabus/#course-overview","title":"COURSE OVERVIEW","text":""},{"location":"syllabus/#course-description","title":"Course Description","text":"<p>In today\u2019s data-driven world, organizations often find themselves rich in data but lacking actionable insights. This course explores the field of data mining, which focuses on extracting meaningful insights from data to support quantitative decision-making. You will get a comprehensive introduction to key data mining concepts and techniques, with an emphasis on their practical application. While the technical details of methodologies will be covered as needed, the primary focus will be on how to effectively use these techniques in real-world scenarios using Python, one of the most popular tools for data analysis and mining. By the end of the course, you will not only develop an appreciation for the vast opportunities within the field of business analytics but also acquire the skills and knowledge needed to leverage these opportunities effectively.</p>"},{"location":"syllabus/#course-details","title":"Course Details","text":"<p>Dates: 3-Feb-2025 - 19-May-2025 Day/Time: Mon 4:00PM - 6:50PM Location: KCBE-104</p>"},{"location":"syllabus/#instructor-information","title":"Instructor Information","text":"<p>Instructor: Chris Young, MBA Email: dcyoung@sandiego.edu Office Hours: Mon 3:30-4:00PM in LC204 or by appointment</p>"},{"location":"syllabus/#course-learning-outcomes","title":"Course Learning Outcomes","text":"<p>At the conclusion of the course, you should be able to:</p> <ul> <li>Understand the principles and concepts of data mining, including its role in business decision-making.</li> <li>Explore key data mining techniques such as classification, clustering, and association rule mining.</li> <li>Gain proficiency in applying data mining algorithms using Python.</li> <li>Analyze and interpret data mining results to provide actionable business insights.</li> </ul>"},{"location":"syllabus/#course-materials","title":"COURSE MATERIALS","text":""},{"location":"syllabus/#required-textbook","title":"Required Textbook","text":"<p>The textbook Data Mining for Business Analytics: Concepts, Techniques and Applications in Python, by Galit Shmueli, Peter C. Bruce et al. (Nov 2019) is required for this course and can be purchased on Amazon here. If you purchase from another online retailer, be sure to purchase the Python edition (ISBN-10:1119549841) as the textbook is published for a variety of statistical software tools. The textbook also has a supplemental website with a variety of Python resources and datasets.</p>"},{"location":"syllabus/#course-resources","title":"Course Resources","text":"<ul> <li>All lecture slides, resources, and assignments are provided on this website or GitHub repository.</li> <li>All homework and programming assignments will be submitted on Canvas/Gradescope.</li> <li>All course announcements will occur through Canvas.</li> </ul>"},{"location":"syllabus/#required-technology","title":"Required Technology","text":"<p>This is a data mining class and we will be primarily using Python and Jupyter Notebooks throughout the course. Below is a list of software that you will need to install on your computer.</p> <ul> <li>Anaconda Distribution of Python and other tools</li> <li>A plain text editor such as VS Code is highly recommended</li> </ul>"},{"location":"syllabus/#course-content-policies","title":"COURSE CONTENT &amp; POLICIES","text":""},{"location":"syllabus/#grade-distribution","title":"Grade Distribution","text":"Assignment % of Total Grade 450 Total Points 6 Conceptual Homework Assignments 20 90 (15 each) 4 Programming Assignments 18 80 (20 each) 1 Hackathon 9 40 1 Group Project 18 80 2 Exams 36 160 (80 each) <p>Please be aware of the following acronyms:</p> <ul> <li>HW = Conceptual homework assignments completed in a document format</li> <li>PA = Programming assignments completed in Python/Jupyter Notebook</li> </ul>"},{"location":"syllabus/#grading-scale","title":"Grading Scale","text":"Final Grade % of 500 Points A &gt;=93% A- 90-92.99% B+ 88-89.99% B 83-87.99% B- 80-82.99% C+ 78-79.99% C 73-77.99% C- 70-72.99% D+ 68-69.99% D 63-67.99% D- 60-62.99% F &lt;60% <p>Assignment and Exam grades are released typically a week after the submission date. It is your responsibility to check your grade and reach out if you believe there is a problem. Final Grades are not rounded up.</p>"},{"location":"syllabus/#conceptual-homework","title":"Conceptual Homework","text":"<p>There are eight (8) conceptual homework assignments that focus on understanding key concepts and solving problems step-by-step by hand, rather than relying on programmatic solutions. While you are allowed to use Python/AI tools to support your learning during homework, it is essential to ensure you fully understand the reasoning and process behind the solutions. AI will not be permitted during exams, so developing your ability to work through problems independently is critical for success. </p> <p>Tip</p> <p>Use these assignments as an opportunity to practice and solidify your understanding, so you\u2019re well-prepared for exam-style questions.</p>"},{"location":"syllabus/#programming-assignments","title":"Programming Assignments","text":"<p>There are four (4) homework assignments consisting of problem-solving and Python based programming assignments. The purpose is to provide students with the opportunity to apply and practice the concepts and skills learned in the lectures and demos. Homeworks may be difficult and require you to demonstrate a deeper understanding of the material.</p>"},{"location":"syllabus/#case-study-and-group-project","title":"Case Study and Group Project","text":"<p>There is one (1) case study and group project that you will be able to collaborate with 1 other classmate. For the case study, you will choose from a list of the cases in the textbook and apply the relevant concepts that we have learned. For the group project, you will pick your own dataset and identify the problem statement and prepare your own analysis and presentation of your findings at the end of the semester.</p>"},{"location":"syllabus/#exams","title":"Exams","text":"<p>Both exams are open-note and completed by yourself with no collaboration/communication with any other students. You will not be able to use a computer during exams. No late exams are permitted, except for extenuating circumstances. Please reach out as early as possible if you know something will prevent you from attending class on exam dates.     </p>"},{"location":"syllabus/#class-attendance","title":"Class Attendance","text":"<p>The goal is to make lectures worth your while to attend. Some class dates may only consist of a lecture but the vast majority of classes will involve hands-on practice with Python and Jupyter Notebooks. Regular attendance is very important to your success in this course. </p>"},{"location":"syllabus/#late-submission-policy","title":"Late Submission Policy","text":"<p>There is a 24-hour grace period for all homework and programming assignments with NO late penalty. Assignment submissions will NOT be accepted after the grace period. This policy is intended to be a safety net in case you experience any difficulties submitting your assignment on time. Do not view the grace period as the true due date for the assignment. If you miss an assignment due date, it is likely that you are not managing your time effectively and will need to adjust your planning and study habits. Please note that any excuse for not submitting assignments on time will not be accepted AFTER the 24-hour grace period. If you have extenuating circumstances, you must contact the professor BEFORE the assignment due date.</p>"},{"location":"syllabus/#regrade-policy","title":"Regrade Policy","text":"<p>The intent of the regrade policy is protect students from serious issues in grading. Email the professor within 72 hours and provide evidence for why your answer is correct and merits a regrade (i.e. a specific reference to something said in a lecture, the readings, or office hours). Make sure you confer with your team first on any group completed project and submit one regrade request after your team comes to a consensus.</p>"},{"location":"syllabus/#school-resources","title":"SCHOOL RESOURCES","text":""},{"location":"syllabus/#academic-honesty-statement","title":"Academic Honesty Statement","text":"<p>USD\u2019s policy on academic integrity is expressly integrated into this course. (Please consult https://www.sandiego.edu/conduct/the-code/rules-of-conduct.php to review this policy.) Any deviation from the standards of this policy may result in a grade of \u201cF\u201d for the course. Because most of the work in this course must be your own, any unauthorized assistance will be considered a violation of the academic integrity policy. If you have questions about the propriety of your work or other participants\u2019 conduct concerning this course, I am readily available to offer an interpretation of this policy.</p>"},{"location":"syllabus/#disability-statement","title":"Disability Statement","text":"<p>It is University of San Diego policy not to discriminate against qualified students with a documented disability in its educational programs, activities or services. If you have a disability-related need for accommodations in this class, contact the Student Affairs office for assistance.</p>"},{"location":"syllabus/#general-student-conduct","title":"General Student Conduct","text":"<p>The University of San Diego School of Business expects its students to conduct themselves in a professional manner at all times. Its students are generally individuals who are preparing for career employment. An integral part of their career and professional development is the expectation that they will conduct themselves during the educational processes in the same manner as will be expected in an employment situation. The University of San Diego Student Code of Rights and Responsibilities is published online at https://www.sandiego.edu/conduct/the-code/.</p>"},{"location":"syllabus/#food-insecurity-pantry","title":"Food Insecurity &amp; Pantry","text":"<p>https://www.meetatusd.com/toreros-against-hunger</p> <p>The goal of Toreros Against Hunger at the University of San Diego is to serve as occasional food relief for University of San Diego students experiencing food insecurity while actively decreasing the amount of food going to waste on campus.</p> <p>Food insecurity broadly defined is \u201cthe state of being without reliable access to sufficient quantity of affordable, nutritious food.\u201d Indicators of food insecurity include skipping meals and/or cutting the size of meals due to lack of financial resources, experiencing hunger but not eating and/or the inability to afford balanced meals.</p> <p>https://www.sandiego.edu/sustainability/initiatives/social-justice.php</p>"},{"location":"syllabus/#counseling-center","title":"Counseling Center","text":"<p>https://www.sandiego.edu/counseling-center/</p> <p>We strive to facilitate students' personal growth and enhance their academic success through accessible, culturally congruent clinical and outreach services. We work in collaboration with other Wellness and university departments and community partners.</p> <p>A counselor-on call is available to consult about after-hours urgent psychological concerns at all times. The counselor-on call can be reached by calling 619-260-4655 (24 hours a day, 7 days a week). Please contact the Department of Public Safety to access emergency services (x2222 on any campus telephone, otherwise call 619-260-2222).</p> <p>The 24-hour San Diego Access and Crisis Line (1-888-724-7240) also offers crisis intervention, information, and referrals.https://www.sandiego.edu/conduct/the-code/rules-of-conduct.php</p>"},{"location":"syllabus/#course-evaluations","title":"Course Evaluations","text":"<p>An online evaluation will be made available to you near the end of this course. Your timely and considered feedback is valuable to us and an important element of your learning experience.</p> <p>Notice</p> <p>This syllabus is subject to change based on the needs of the class; I will make sure to notify you through an announcement on Canvas.</p>"},{"location":"notes/week_1/","title":"Week 1 - Intro / Association Rules","text":"<p>Learning Objectives</p> <p>After today's class, you should be able to:</p> <ul> <li>Familiarize yourself with the foundational concepts of data mining and effectively navigate the course website and materials to access the syllabus, calendar, lectures, and assignments.</li> <li>Understand and apply association rule mining techniques to identify patterns, relationships, and correlations within large datasets, including the concepts of support, confidence, and lift.</li> </ul>"},{"location":"notes/week_1/#class-agenda","title":"Class Agenda","text":"<ul> <li>Course Overview</li> <li>Introduction Slides</li> <li><code>[5 min]</code> Break</li> <li>Association Rules Slides</li> </ul>"},{"location":"notes/week_1/#task-list","title":"Task List","text":"<ul> <li> Optional Reading: Chapters 1-2 - Introduction and Overview of Data Mining</li> <li> Required Reading: Chapters 14.1 - Association Rules</li> <li> Complete and Submit HW1_Association_Rules on Canvas/Gradescope.</li> </ul>"},{"location":"notes/week_1/#additional-resources","title":"Additional Resources","text":"<ul> <li>Chapter 14 Code Examples</li> </ul>"},{"location":"notes/week_2/","title":"Week 2 - Collaborative Filtering","text":"<p>Learning Objectives</p> <p>After today's class, you should be able to:</p> <ul> <li>Create a new Conda environment and install required Python libraries.</li> <li>Understand and apply collaborative filtering techniques, including user-based and item-based approaches, to generate personalized recommendations by leveraging user preferences, behaviors, and interactions.</li> <li>Develop and implement association rule mining and collaborative filtering techniques in Python.</li> </ul>"},{"location":"notes/week_2/#class-agenda","title":"Class Agenda","text":"<ul> <li>Python Conda Environment Setup</li> <li>Association Rules Python Demo</li> <li><code>[5 min]</code> Break</li> <li>Collaborative Filtering Slides</li> <li>Collaborative Filtering Python Demo</li> </ul>"},{"location":"notes/week_2/#task-list","title":"Task List","text":"<ul> <li> Required Reading: Chapters 14.2 - Collaborative Filtering</li> <li> Complete and Submit PA1_Instacart_Recommendations on Canvas/Gradescope.</li> </ul>"},{"location":"notes/week_2/#additional-resources","title":"Additional Resources","text":"<ul> <li>Chapter 14 Code Examples</li> </ul>"},{"location":"notes/week_3/","title":"Week 3 - Cluster Analysis","text":"<p>Learning Objectives</p> <p>After today's class, you should be able to:</p> <ul> <li>Understand and apply cluster analysis techniques, including the k-Means algorithm, to segment data into a set of homogeneous clusters of records for the purpose of generating insight.</li> <li>Use GitHub Desktop to clone a repository and download starter Jupyter notebooks for in class demos.</li> <li>Develop and implement cluster analysis models in Python.</li> </ul>"},{"location":"notes/week_3/#class-agenda","title":"Class Agenda","text":"<ul> <li>Cluster Analysis Slides</li> <li><code>[5 min]</code> Break</li> <li>Cluster Analysis Python Demo</li> <li><code>[30 min]</code> Python Q&amp;A/Debugging Help</li> </ul>"},{"location":"notes/week_3/#task-list","title":"Task List","text":"<ul> <li> Optional Reading: Chapters 3-4 - Data Visualization and Dimension Reduction</li> <li> Required Reading: Chapters 15 - Cluster Analysis</li> <li> Complete and Submit HW2_Cluster_Analysis on Canvas/Gradescope.</li> <li> Complete and Submit PA1_Instacart_Recommendations on Canvas/Gradescope.</li> </ul>"},{"location":"notes/week_3/#additional-resources","title":"Additional Resources","text":"<ul> <li>Chapter 15 Code Examples</li> </ul>"},{"location":"resources/python/cl_cheatsheet/","title":"Command Line Cheatsheet","text":"Command Line Notes <code>conda list</code> Uses conda to list out all installed packages, version, build and how it was installed in the current environment. <code>conda search --full-name python</code> Searches conda channels for the current versions of python that are available. <code>conda env list</code> List out all conda environments on computer. <code>conda create -n myenv python=3.12.7</code> Create a new environment with the specified version of Python and minimal packages. Replace <code>myenv</code> with the desired name of the new environment and <code>3.12.7</code> with the desired Python version. <code>conda env remove -n myenv</code> Remove environment from your computer. You cannot have the current environment activated. Replace <code>myenv</code> with the name of the environment that you want to remove. \u26a0\ufe0fWarning \u26a0\ufe0f this deletes the env folder from your computer! <code>conda deactivate</code> Deactivate current environment. <code>conda activate myenv</code> Activate specified environment. Replace <code>myenv</code> with the name of the environment that you want to activate. <code>pip list</code> Use pip to list installed packages and version. Similar to conda list. <code>pip install pandas</code> Use pip to install the current version of a package in the current environment. Replace <code>pandas</code> with the name of the package that you want to install. This will also install all dependencies. <code>pip install pandas==2.0</code> Use pip to install a specific version of a package in the current environment. <code>pip install \"pandas&lt;2.0\"</code> Use pip to install the version less than a specific version of a package in the current environment. \u2139\ufe0f Double Quotes are required. <code>pip install --upgrade pandas</code> Use pip to upgrade to the current version of a package in the current environment. <code>pip uninstall pandas</code> Use pip to uninstall a package from the current environment."},{"location":"resources/textbook/ch14_code_examples/","title":"Chapter 14","text":""},{"location":"resources/textbook/ch14_code_examples/#chapter-14-association-rules-and-collaborative-filtering","title":"Chapter 14 Association Rules and Collaborative Filtering","text":"<p>Original Code Credit:: Shmueli, Galit; Bruce, Peter C.; Gedeck, Peter; Patel, Nitin R.. Data Mining for Business Analytics Wiley.</p> <p>Modifications have been made from the original textbook examples due to version changes in library dependencies and/or for clarity.</p> <p>Download this notebook and data here.</p>"},{"location":"resources/textbook/ch14_code_examples/#import-libraries","title":"Import Libraries","text":"<pre><code>import os\nimport heapq\nimport random\nfrom collections import defaultdict\nimport pandas as pd\n\nimport matplotlib.pylab as plt\nfrom mlxtend.frequent_patterns import apriori, association_rules\nfrom surprise import Dataset, Reader, KNNBasic\nfrom surprise.model_selection import train_test_split\n</code></pre>"},{"location":"resources/textbook/ch14_code_examples/#141-association-rules","title":"14.1 Association Rules","text":""},{"location":"resources/textbook/ch14_code_examples/#example-1-synthetic-data-on-purchases-of-phone-faceplates","title":"Example 1: Synthetic Data on Purchases of Phone Faceplates","text":"<pre><code># Load and preprocess data set \nfp_df = pd.read_csv(os.path.join('data', 'Faceplate.csv'))\nfp_df.set_index('Transaction', inplace=True)\nfp_df = fp_df.astype(bool, 0)\n\n# create frequent itemsets\nitemsets = apriori(fp_df, min_support=0.2, use_colnames=True)\n\n# convert into rules\nrules = association_rules(itemsets, num_itemsets=len(fp_df), metric='confidence', min_threshold=0.5)\nrules.sort_values(by=['lift'], ascending=False).head(6)\n</code></pre> antecedents consequents antecedent support consequent support support confidence lift representativity leverage conviction zhangs_metric jaccard certainty kulczynski 12 (Red, White) (Green) 0.4 0.2 0.2 0.5 2.500000 1.0 0.12 1.6 1.000 0.500000 0.375 0.750000 15 (Green) (Red, White) 0.2 0.4 0.2 1.0 2.500000 1.0 0.12 inf 0.750 0.500000 1.000 0.750000 4 (Green) (Red) 0.2 0.6 0.2 1.0 1.666667 1.0 0.08 inf 0.500 0.333333 1.000 0.666667 14 (White, Green) (Red) 0.2 0.6 0.2 1.0 1.666667 1.0 0.08 inf 0.500 0.333333 1.000 0.666667 7 (Orange) (White) 0.2 0.7 0.2 1.0 1.428571 1.0 0.06 inf 0.375 0.285714 1.000 0.642857 8 (Green) (White) 0.2 0.7 0.2 1.0 1.428571 1.0 0.06 inf 0.375 0.285714 1.000 0.642857"},{"location":"resources/textbook/ch14_code_examples/#example-2-rules-for-similar-book-purchases","title":"Example 2: Rules for Similar Book Purchases","text":"<pre><code># load dataset\nall_books_df = pd.read_csv(os.path.join('data', 'CharlesBookClub.csv'))\nignore = ['Seq#', 'ID#', 'Gender', 'M', 'R', 'F', 'FirstPurch', 'Related Purchase',\n          'Mcode', 'Rcode', 'Fcode', 'Yes_Florence', 'No_Florence']\ncount_books = all_books_df.drop(columns=ignore)\ncount_books[count_books &gt; 0] = 1\ncount_books = count_books.astype(bool, 0)\n# create frequent itemsets and rules\nitemsets = apriori(count_books, min_support=200/4000, use_colnames=True)\nrules = association_rules(itemsets, num_itemsets=len(count_books), metric='confidence', min_threshold=0.5)\nrules.sort_values(by=['lift'], ascending=False).head(10)\n</code></pre> antecedents consequents antecedent support consequent support support confidence lift representativity leverage conviction zhangs_metric jaccard certainty kulczynski 64 (YouthBks, RefBks) (ChildBks, CookBks) 0.08125 0.24200 0.05525 0.680000 2.809917 1.0 0.035588 2.368750 0.701080 0.206157 0.577836 0.454153 73 (RefBks, DoItYBks) (ChildBks, CookBks) 0.09250 0.24200 0.06125 0.662162 2.736207 1.0 0.038865 2.243680 0.699207 0.224154 0.554304 0.457631 60 (YouthBks, DoItYBks) (ChildBks, CookBks) 0.10325 0.24200 0.06700 0.648910 2.681448 1.0 0.042014 2.158993 0.699266 0.240791 0.536821 0.462885 80 (GeogBks, RefBks) (ChildBks, CookBks) 0.08175 0.24200 0.05025 0.614679 2.539995 1.0 0.030467 1.967190 0.660276 0.183729 0.491661 0.411162 69 (GeogBks, YouthBks) (ChildBks, CookBks) 0.10450 0.24200 0.06325 0.605263 2.501087 1.0 0.037961 1.920267 0.670211 0.223301 0.479239 0.433313 77 (GeogBks, DoItYBks) (ChildBks, CookBks) 0.10100 0.24200 0.06050 0.599010 2.475248 1.0 0.036058 1.890321 0.662959 0.214159 0.470989 0.424505 67 (GeogBks, ChildBks, CookBks) (YouthBks) 0.10950 0.23825 0.06325 0.577626 2.424452 1.0 0.037162 1.803495 0.659782 0.222320 0.445521 0.421552 70 (ChildBks, RefBks, CookBks) (DoItYBks) 0.10350 0.25475 0.06125 0.591787 2.323013 1.0 0.034883 1.825642 0.635276 0.206229 0.452247 0.416110 48 (GeogBks, DoItYBks) (YouthBks) 0.10100 0.23825 0.05450 0.539604 2.264864 1.0 0.030437 1.654554 0.621215 0.191396 0.395607 0.384178 63 (ChildBks, RefBks, CookBks) (YouthBks) 0.10350 0.23825 0.05525 0.533816 2.240573 1.0 0.030591 1.634013 0.617608 0.192845 0.388010 0.382858"},{"location":"resources/textbook/ch14_code_examples/#142-collaborative-filtering","title":"14.2 Collaborative Filtering","text":""},{"location":"resources/textbook/ch14_code_examples/#example-3-netflix-prize-contest","title":"Example 3: Netflix Prize Contest","text":"<pre><code>random.seed(0)\nnratings = 5000\nrandomData = pd.DataFrame({\n    'itemID': [random.randint(0,99) for _ in range(nratings)],\n    'userID': [random.randint(0,999) for _ in range(nratings)],\n    'rating': [random.randint(1,5) for _ in range(nratings)]\n})\ndef get_top_n(predictions, n=10):\n    # First map the predictions to each user.\n    byUser = defaultdict(list)\n    for p in predictions:\n        byUser[p.uid].append(p)\n\n    # For each user, reduce predictions to top-n\n    for uid, userPredictions in byUser.items():\n        byUser[uid] = heapq.nlargest(n, userPredictions, key=lambda p: p.est)\n    return byUser\n</code></pre> <pre><code># Convert the data set into the format required by the surprise package\n# The columns must correspond to user id, item id, and ratings (in that order)\nreader = Reader(rating_scale=(1, 5))\ndata = Dataset.load_from_df(randomData[['userID', 'itemID', 'rating']], reader)\n# Split into training and test set\ntrainset, testset = train_test_split(data, test_size=.25, random_state=1)\n## User-based filtering\n# compute cosine similarity between users \nsim_options = {'name': 'cosine', 'user_based': True}\nalgo = KNNBasic(sim_options=sim_options)\nalgo.fit(trainset)\n# predict ratings for all pairs (u, i) that are NOT in the training set.\npredictions = algo.test(testset) \n# Print the recommended items for each user\ntop_n = get_top_n(predictions, n=4)\nprint('Top-4 recommended items for each user')\nfor uid, user_ratings in list(top_n.items())[:5]:\n    print('User {}'.format(uid))\n    for prediction in user_ratings:\n        print('  Item {0.iid} ({0.est:.2f})'.format(prediction), end='')\n    print()\n</code></pre> <pre><code>Computing the cosine similarity matrix...\nDone computing similarity matrix.\nTop-4 recommended items for each user\nUser 6\n  Item 6 (5.00)  Item 77 (2.50)  Item 60 (1.00)\nUser 222\n  Item 77 (3.50)  Item 75 (2.78)\nUser 424\n  Item 14 (3.50)  Item 45 (3.10)  Item 54 (2.34)\nUser 87\n  Item 27 (3.00)  Item 54 (3.00)  Item 82 (3.00)  Item 32 (1.00)\nUser 121\n  Item 98 (3.48)  Item 32 (2.83)\n</code></pre> <pre><code>trainset = data.build_full_trainset()\nsim_options = {'name': 'cosine', 'user_based': False}\nalgo = KNNBasic(sim_options=sim_options)\nalgo.fit(trainset)\n# Predict rating for user 383 and item 7\nalgo.predict(383, 7)\n</code></pre> <pre><code>Computing the cosine similarity matrix...\nDone computing similarity matrix.\n\n\n\n\n\nPrediction(uid=383, iid=7, r_ui=None, est=2.3661840936304324, details={'actual_k': 4, 'was_impossible': False})\n</code></pre> <pre><code>\n</code></pre>"},{"location":"resources/textbook/ch15_code_examples/","title":"Chapter 15","text":""},{"location":"resources/textbook/ch15_code_examples/#chapter-15-cluster-analysis","title":"Chapter 15 Cluster Analysis","text":"<p>Original Code Credit:: Shmueli, Galit; Bruce, Peter C.; Gedeck, Peter; Patel, Nitin R.. Data Mining for Business Analytics Wiley.</p> <p>Modifications have been made from the original textbook examples due to version changes in library dependencies and/or for clarity.</p> <p>Download this notebook and data here.</p>"},{"location":"resources/textbook/ch15_code_examples/#import-libraries","title":"Import Libraries","text":"<pre><code>import os\nimport pandas as pd\nfrom sklearn import preprocessing\nfrom sklearn.metrics import pairwise\nfrom scipy.cluster.hierarchy import dendrogram, linkage, fcluster\nfrom sklearn.cluster import KMeans\nimport matplotlib.pylab as plt\nimport seaborn as sns\nfrom pandas.plotting import parallel_coordinates\n</code></pre>"},{"location":"resources/textbook/ch15_code_examples/#151-introduction","title":"15.1 Introduction","text":""},{"location":"resources/textbook/ch15_code_examples/#example-public-utilities","title":"Example: Public Utilities","text":"<pre><code>utilities_df = pd.read_csv(os.path.join('data', 'Utilities.csv'))\n# set row names to the utilities column\nutilities_df.set_index('Company', inplace=True)\n# while not required, the conversion of integer data to float\n# will avoid a warning when applying the scale function\nutilities_df = utilities_df.apply(lambda x: x.astype('float64'))\n# compute Euclidean distance\nd = pairwise.pairwise_distances(utilities_df, metric='euclidean')\npd.DataFrame(d, columns=utilities_df.index, index=utilities_df.index)\n</code></pre> Company Arizona Boston Central Commonwealth NY Florida Hawaiian Idaho Kentucky Madison ... Northern Oklahoma Pacific Puget San Diego Southern Texas Wisconsin United Virginia Company Arizona 0.000000 3989.408076 140.402855 2654.277632 5777.167672 2050.529440 1435.265019 4006.104187 671.276346 2622.699002 ... 1899.279821 598.556633 2609.045363 6914.742065 3363.061626 1063.009074 4430.251585 1790.485648 2427.588875 1016.617691 Boston 3989.408076 0.000000 4125.044132 1335.466502 1788.068027 6039.689076 2554.287162 7994.155985 3318.276558 1367.090634 ... 2091.160485 4586.302564 1380.749962 10903.146464 629.760748 5052.331669 8419.610541 2199.721665 1562.210811 5005.081262 Central 140.402855 4125.044132 0.000000 2789.759674 5912.552908 1915.155154 1571.295401 3872.257626 807.920792 2758.559663 ... 2035.441520 461.341670 2744.502847 6780.430307 3498.113013 928.749249 4295.014690 1925.772564 2563.637362 883.535455 Commonwealth 2654.277632 1335.466502 2789.759674 0.000000 3123.153215 4704.363099 1219.560005 6659.534567 1983.314354 43.648894 ... 756.831954 3250.984589 56.644626 9568.434429 710.292965 3717.202963 7084.372839 864.273153 232.476871 3670.018191 NY 5777.167672 1788.068027 5912.552908 3123.153215 0.000000 7827.429211 4342.093798 9782.158178 5106.094153 3155.095594 ... 3879.167462 6373.743249 3168.177463 12691.155108 2414.698757 6840.150291 10207.392630 3987.335962 3350.073118 6793.035300 Florida 2050.529440 6039.689076 1915.155154 4704.363099 7827.429211 0.000000 3485.671562 1959.731080 2721.706296 4672.829286 ... 3949.092316 1454.292604 4659.356262 4866.111649 5413.093004 988.044559 2380.124974 3840.227943 4478.028874 1035.981475 Hawaiian 1435.265019 2554.287162 1571.295401 1219.560005 4342.093798 3485.671562 0.000000 5440.461781 764.083188 1187.941143 ... 466.559118 2032.614245 1174.075616 8349.366438 1928.441480 2498.149024 5865.447190 358.476293 992.453252 2451.185161 Idaho 4006.104187 7994.155985 3872.257626 6659.534567 9782.158178 1959.731080 5440.461781 0.000000 4676.638384 6627.291780 ... 5903.395450 3412.263965 6614.499239 2909.014679 7368.815437 2943.535570 447.828673 5795.958815 6432.132202 2989.963982 Kentucky 671.276346 3318.276558 807.920792 1983.314354 5106.094153 2721.706296 764.083188 4676.638384 0.000000 1951.628580 ... 1228.436327 1269.102099 1938.026557 7585.467294 2692.212361 1734.103297 5101.414140 1119.940014 1756.378966 1687.236030 Madison 2622.699002 1367.090634 2758.559663 43.648894 3155.095594 4672.829286 1187.941143 6627.291780 1951.628580 0.000000 ... 724.096182 3219.825109 53.301401 9536.242192 744.253668 3685.510088 7052.723883 833.472995 199.228400 3638.097548 Nevada 8364.031051 12353.062698 8229.223281 11018.057812 14141.022579 6314.359092 9799.015552 4359.599605 9035.007488 10986.098011 ... 10262.157285 7768.384793 10973.010950 1452.162005 11727.066293 7301.040864 3934.617521 10154.118793 10791.049271 7348.049019 New England 2923.136103 1066.579432 3058.707429 271.452731 2854.099482 4973.506840 1488.014909 6928.326174 2252.026717 304.277034 ... 1026.482994 3519.977565 314.354030 9837.281834 442.132760 3986.102433 7353.379146 1134.145010 496.687413 3939.100355 Northern 1899.279821 2091.160485 2035.441520 756.831954 3879.167462 3949.092316 466.559118 5903.395450 1228.436327 724.096182 ... 0.000000 2496.638890 713.665046 8812.303559 1466.991954 2961.834750 6328.917948 119.981262 531.476328 2914.204993 Oklahoma 598.556633 4586.302564 461.341670 3250.984589 6373.743249 1454.292604 2032.614245 3412.263965 1269.102099 3219.825109 ... 2496.638890 0.000000 3205.748876 6319.933836 3959.240748 470.164792 3834.012257 2386.942751 3024.952355 428.065259 Pacific 2609.045363 1380.749962 2744.502847 56.644626 3168.177463 4659.356262 1174.075616 6614.499239 1938.026557 53.301401 ... 713.665046 3205.748876 0.000000 9523.413499 754.612093 3672.035402 7039.262070 820.164297 186.388651 3625.118869 Puget 6914.742065 10903.146464 6780.430307 9568.434429 12691.155108 4866.111649 8349.366438 2909.014679 7585.467294 9536.242192 ... 8812.303559 6319.933836 9523.413499 0.000000 10277.660378 5851.893307 2488.432223 8704.721278 9341.126615 5898.576962 San Diego 3363.061626 629.760748 3498.113013 710.292965 2414.698757 5413.093004 1928.441480 7368.815437 2692.212361 744.253668 ... 1466.991954 3959.240748 754.612093 10277.660378 0.000000 4426.041889 7793.083947 1573.408379 938.522726 4379.211818 Southern 1063.009074 5052.331669 928.749249 3717.202963 6840.150291 988.044559 2498.149024 2943.535570 1734.103297 3685.510088 ... 2961.834750 470.164792 3672.035402 5851.893307 4426.041889 0.000000 3367.318870 2853.298778 3490.422918 59.325286 Texas 4430.251585 8419.610541 4295.014690 7084.372839 10207.392630 2380.124974 5865.447190 447.828673 5101.414140 7052.723883 ... 6328.917948 3834.012257 7039.262070 2488.432223 7793.083947 3367.318870 0.000000 6220.296729 6857.735864 3414.831455 Wisconsin 1790.485648 2199.721665 1925.772564 864.273153 3987.335962 3840.227943 358.476293 5795.958815 1119.940014 833.472995 ... 119.981262 2386.942751 820.164297 8704.721278 1573.408379 2853.298778 6220.296729 0.000000 640.786770 2806.165712 United 2427.588875 1562.210811 2563.637362 232.476871 3350.073118 4478.028874 992.453252 6432.132202 1756.378966 199.228400 ... 531.476328 3024.952355 186.388651 9341.126615 938.522726 3490.422918 6857.735864 640.786770 0.000000 3443.240967 Virginia 1016.617691 5005.081262 883.535455 3670.018191 6793.035300 1035.981475 2451.185161 2989.963982 1687.236030 3638.097548 ... 2914.204993 428.065259 3625.118869 5898.576962 4379.211818 59.325286 3414.831455 2806.165712 3443.240967 0.000000 <p>22 rows \u00d7 22 columns</p> <pre><code># pandas uses sample standard deviation\nutilities_df_norm = (utilities_df - utilities_df.mean())/utilities_df.std()\n# compute normalized distance based on Sales and Fuel Cost\nutilities_df_norm[['Sales', 'Fuel_Cost']]\nd_norm = pairwise.pairwise_distances(utilities_df_norm[['Sales', 'Fuel_Cost']],\n                                     metric='euclidean')\npd.DataFrame(d_norm, columns=utilities_df.index, index=utilities_df.index)\n</code></pre> Company Arizona Boston Central Commonwealth NY Florida Hawaiian Idaho Kentucky Madison ... Northern Oklahoma Pacific Puget San Diego Southern Texas Wisconsin United Virginia Company Arizona 0.000000 2.010329 0.774179 0.758738 3.021907 1.244422 1.885248 1.265638 0.461292 0.738650 ... 0.564657 0.182648 1.570780 1.947668 2.509043 0.913621 1.247976 0.521491 2.761745 1.252350 Boston 2.010329 0.000000 1.465703 1.582821 1.013370 1.792397 0.740283 3.176654 1.557738 1.719632 ... 1.940166 2.166078 0.478334 3.501390 0.679634 1.634425 2.890560 1.654255 1.100595 1.479261 Central 0.774179 1.465703 0.000000 1.015710 2.432528 0.631892 1.156092 1.732777 0.419254 1.102287 ... 1.113433 0.855093 0.987772 2.065643 1.836762 0.276440 1.428159 0.838967 2.034824 0.510365 Commonwealth 0.758738 1.582821 1.015710 0.000000 2.571969 1.643857 1.746027 2.003230 0.629994 0.138758 ... 0.377004 0.937389 1.258835 2.699060 2.202930 1.278514 1.998818 0.243408 2.547116 1.502093 NY 3.021907 1.013370 2.432528 2.571969 0.000000 2.635573 1.411695 4.162561 2.566439 2.705445 ... 2.938637 3.174588 1.462019 4.397433 0.715629 2.558409 3.831132 2.661786 0.952507 2.328691 Florida 1.244422 1.792397 0.631892 1.643857 2.635573 0.000000 1.228805 1.764123 1.025663 1.722510 ... 1.698624 1.243634 1.343185 1.767581 1.953423 0.366744 1.277920 1.452417 2.016493 0.313847 Hawaiian 1.885248 0.740283 1.156092 1.746027 1.411695 1.228805 0.000000 2.860189 1.436822 1.880361 ... 2.027224 1.997036 0.560997 2.995848 0.726095 1.205034 2.463227 1.711256 0.879934 0.929414 Idaho 1.265638 3.176654 1.732777 2.003230 4.162561 1.764123 2.860189 0.000000 1.650417 1.950296 ... 1.708409 1.083449 2.705579 0.992092 3.563727 1.658671 0.600089 1.778813 3.720421 1.980715 Kentucky 0.461292 1.557738 0.419254 0.629994 2.566439 1.025663 1.436822 1.650417 0.000000 0.697674 ... 0.694524 0.608401 1.110854 2.180496 2.048098 0.658996 1.493274 0.426780 2.308613 0.929141 Madison 0.738650 1.719632 1.102287 0.138758 2.705445 1.722510 1.880361 1.950296 0.697674 0.000000 ... 0.267198 0.908665 1.397240 2.686215 2.341644 1.355786 1.986625 0.274061 2.685340 1.599587 Nevada 2.369479 3.756513 2.375975 3.106084 4.597006 1.971518 3.185311 1.479526 2.550689 3.105627 ... 2.923023 2.211990 3.293310 0.487508 3.899212 2.145585 1.133311 2.862756 3.887918 2.284803 New England 2.425975 0.684393 1.737322 2.153831 0.846291 1.831380 0.608107 3.458771 1.966323 2.292531 ... 2.480456 2.554109 0.898094 3.598846 0.130663 1.809354 3.071178 2.172473 0.417866 1.536436 Northern 0.564657 1.940166 1.113433 0.377004 2.938637 1.698624 2.027224 1.708409 0.694524 0.267198 ... 0.000000 0.711050 1.582591 2.487892 2.538720 1.336887 1.793287 0.316160 2.861293 1.623614 Oklahoma 0.182648 2.166078 0.855093 0.937389 3.174588 1.243634 1.997036 1.083449 0.608401 0.908665 ... 0.711050 0.000000 1.716739 1.780656 2.642155 0.944295 1.083449 0.702684 2.876646 1.296548 Pacific 1.570780 0.478334 0.987772 1.258835 1.462019 1.343185 0.560997 2.705579 1.110854 1.397240 ... 1.582591 1.716739 0.000000 3.027116 0.958905 1.160017 2.412278 1.276200 1.288563 1.035028 Puget 1.947668 3.501390 2.065643 2.699060 4.397433 1.767581 2.995848 0.992092 2.180496 2.686215 ... 2.487892 1.780656 3.027116 0.000000 3.720970 1.867235 0.700313 2.456272 3.763066 2.069314 San Diego 2.509043 0.679634 1.836762 2.202930 0.715629 1.953423 0.726095 3.563727 2.048098 2.341644 ... 2.538720 2.642155 0.958905 3.720970 0.000000 1.920035 3.185942 2.234632 0.440163 1.655498 Southern 0.913621 1.634425 0.276440 1.278514 2.558409 0.366744 1.205034 1.658671 0.658996 1.355786 ... 1.336887 0.944295 1.160017 1.867235 1.920035 0.000000 1.272784 1.085774 2.062067 0.356298 Texas 1.247976 2.890560 1.428159 1.998818 3.831132 1.277920 2.463227 0.600089 1.493274 1.986625 ... 1.793287 1.083449 2.412278 0.700313 3.185942 1.272784 0.000000 1.756136 3.288460 1.541576 Wisconsin 0.521491 1.654255 0.838967 0.243408 2.661786 1.452417 1.711256 1.778813 0.426780 0.274061 ... 0.316160 0.702684 1.276200 2.456272 2.234632 1.085774 1.756136 0.000000 2.549040 1.343306 United 2.761745 1.100595 2.034824 2.547116 0.952507 2.016493 0.879934 3.720421 2.308613 2.685340 ... 2.861293 2.876646 1.288563 3.763066 0.440163 2.062067 3.288460 2.549040 0.000000 1.749930 Virginia 1.252350 1.479261 0.510365 1.502093 2.328691 0.313847 0.929414 1.980715 0.929141 1.599587 ... 1.623614 1.296548 1.035028 2.069314 1.655498 0.356298 1.541576 1.343306 1.749930 0.000000 <p>22 rows \u00d7 22 columns</p>"},{"location":"resources/textbook/ch15_code_examples/#154-hierarchical-agglomerative-clustering","title":"15.4 Hierarchical (Agglomerative) Clustering","text":"<p>We will focus on kMeans Clustering but code examples for hierarchical clustering are provided for example purposes.</p> <pre><code>fig, axes = plt.subplots(2, 1, figsize=(10,10))\n# in linkage() set argument method =\n# 'single', 'complete', 'average', 'weighted', centroid', 'median', 'ward'\nZ = linkage(utilities_df_norm, method='single')\nax1 = axes[0]\ndendrogram(Z, labels=utilities_df_norm.index, color_threshold=2.75, ax=ax1)\nax1.set_title('Hierarchical Clustering Dendrogram (Single Linkage)')\nZ = linkage(utilities_df_norm, method='average')\nax2 = axes[1]\ndendrogram(Z, labels=utilities_df_norm.index, color_threshold=3.6, ax=ax2)\nax2.set_title('Hierarchical Clustering Dendrogram (Average Linkage)')\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p> <pre><code>memb = fcluster(linkage(utilities_df_norm, method='single'), 6, criterion='maxclust')\nmemb = pd.Series(memb, index=utilities_df_norm.index)\nfor key, item in memb.groupby(memb):\n    print(key, ': ', ', '.join(item.index))\n</code></pre> <pre><code>1 :  Idaho, Puget\n2 :  Arizona , Boston , Commonwealth, Florida , Hawaiian , Kentucky, Madison , New England, Northern, Oklahoma, Pacific , Southern, Texas, Wisconsin, United, Virginia\n3 :  Central \n4 :  San Diego\n5 :  Nevada\n6 :  NY\n</code></pre> <pre><code>memb = fcluster(linkage(utilities_df_norm, method='average'), 6, criterion='maxclust')\nmemb = pd.Series(memb, index=utilities_df_norm.index)\nfor key, item in memb.groupby(memb):\n    print(key, ': ', ', '.join(item.index))\n</code></pre> <pre><code>1 :  Idaho, Nevada, Puget\n2 :  Hawaiian , New England, Pacific , United\n3 :  San Diego\n4 :  Boston , Commonwealth, Madison , Northern, Wisconsin, Virginia\n5 :  Arizona , Central , Florida , Kentucky, Oklahoma, Southern, Texas\n6 :  NY\n</code></pre> <pre><code># set labels as cluster membership and utility name\nutilities_df_norm.index = ['{}: {}'.format(cluster, state)\n                           for cluster, state in zip(memb, utilities_df_norm.index)]\n# plot heatmap\n# the '_r' suffix reverses the color mapping to large = dark\nsns.clustermap(utilities_df_norm, method='average', col_cluster=False, cmap='mako_r')\nplt.show()\n</code></pre> <p></p>"},{"location":"resources/textbook/ch15_code_examples/#155-non-hierarchical-clustering-the-k-means-algorithm","title":"15.5 Non-Hierarchical Clustering: The k-Means Algorithm","text":"<pre><code># Normalize distances\nutilities_df_norm = utilities_df.apply(preprocessing.scale, axis=0)\nkmeans = KMeans(n_clusters=6, init='k-means++', max_iter=300, n_init=10, random_state=0).fit(utilities_df_norm)\n# Cluster membership\nmemb = pd.Series(kmeans.labels_, index=utilities_df_norm.index)\nfor key, item in memb.groupby(memb):\n     print(key, ': ', ', '.join(item.index))\n</code></pre> <pre><code>0 :  Idaho, Puget\n1 :  Arizona , Central , Florida , Kentucky, Oklahoma, Southern, Texas\n2 :  Commonwealth, Madison , Northern, Wisconsin, Virginia\n3 :  Boston , Hawaiian , New England, Pacific , San Diego, United\n4 :  Nevada\n5 :  NY\n</code></pre> <pre><code>centroids = pd.DataFrame(kmeans.cluster_centers_, columns=utilities_df_norm.columns)\ncentroids\n</code></pre> Fixed_charge RoR Cost Load_factor Demand_growth Sales Nuclear Fuel_Cost 0 0.088252 -0.541112 1.995766 -0.109502 0.987702 1.621068 -0.731447 -1.174696 1 0.516184 0.797896 -1.009097 -0.345490 -0.501098 0.360140 -0.535523 -0.420198 2 -0.011599 0.339180 0.224086 -0.366466 0.170386 -0.411331 1.601868 -0.609460 3 -0.632893 -0.639936 0.206692 1.175321 0.057691 -0.757719 -0.380962 1.203616 4 -2.019709 -1.476137 0.119723 -1.256665 1.069762 2.458495 -0.731447 -0.616086 5 2.085268 -0.883194 0.591840 -1.325495 -0.735555 -1.618644 0.219434 1.732470 <pre><code># calculate the distances of each data point to the cluster centers\ndistances = kmeans.transform(utilities_df_norm)\n# find closest cluster for each data point\nminSquaredDistances = distances.min(axis=1) ** 2\n# combine with cluster labels into a data frame\ndf = pd.DataFrame({'squaredDistance': minSquaredDistances, 'cluster': kmeans.labels_},\n    index=utilities_df_norm.index)\n# group by cluster and print information\nfor cluster, data in df.groupby('cluster'):\n    count = len(data)\n    withinClustSS = data.squaredDistance.sum()\n    print(f'Cluster {cluster} ({count} members): {withinClustSS:.2f} within cluster ')\n</code></pre> <pre><code>Cluster 0 (2 members): 2.54 within cluster \nCluster 1 (7 members): 27.77 within cluster \nCluster 2 (5 members): 10.66 within cluster \nCluster 3 (6 members): 22.20 within cluster \nCluster 4 (1 members): 0.00 within cluster \nCluster 5 (1 members): 0.00 within cluster\n</code></pre> <pre><code>centroids['cluster'] = ['Cluster {}'.format(i) for i in centroids.index]\nplt.figure(figsize=(10,6))\nparallel_coordinates(centroids, class_column='cluster', colormap='Dark2', linewidth=5)\nplt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\nplt.show()\n</code></pre> <pre><code>inertia = []\nfor n_clusters in range(1, 7):\n    kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(utilities_df_norm)\n    inertia.append(kmeans.inertia_ / n_clusters)\ninertias = pd.DataFrame({'n_clusters': range(1, 7), 'inertia': inertia})\nax = inertias.plot(x='n_clusters', y='inertia')\nplt.xlabel('Number of clusters (k)')\nplt.ylabel('Average Within-Cluster Squared Distances')\nplt.ylim((0, 1.1 * inertias.inertia.max()))\nax.legend().set_visible(False)\nplt.show()\n</code></pre> <pre><code>\n</code></pre>"}]}